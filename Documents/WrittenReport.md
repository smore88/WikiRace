## Breadth First Search (BFS)
The main idea of the BFS algorithm is that if a traversal is possible between a starting node and an ending node we output “true” in the output file otherwise we return “false” in the output file.

There are two functions that implement the BFS algorithm, the first being IsAccessible and the second being IsAccessibleString. Both functions take an output file, a starting node, and a destination node. The IsAccessible function just checks if it is possible to travel from a starting node to an ending node where the nodes are ints. Then in the IsAccessibleString function, we convert the integer nodes into corresponding string hyperlinks and then run IsAccessible to check if it is possible to go from a starting string node to an ending string node.

We confirmed that the search traversal is working as intended by adding test cases using a “small” and “medium” dataset that our team created whose results we can manually confirm for both functions IsAccessible and IsAccessibleString. We also wrote test cases for the full dataset but since the data file was so big with over 1.7 million nodes, we couldn’t write all the possible combinations of starting nodes to ending nodes so we only wrote general tests cases where we handpicked nodes from the full dataset that were part of disconnected components, cycles, and just general traversals. All of which proves that the algorithm works.


## Dijkstra’s
The Dijkstra’s algorithm takes in the path to output file, the starting node (as name or ID), and the destination node (as name or ID), and then output the shortest path between the two nodes. Moreover, we also write the result to the specified output file. Note that the function shortest_path will run Dijkstra’s algorithm and then store the result of each call to Dijkstra’s algorithm, so that when the same source node is inputted, our program can just fetch the result from the previous call, instead of running the algorithm again. The purpose of Dijkstra’s algorithm in this project is to identify the shortest path between any two given Wikipedia pages, so that the participants of WikiRace can complete the challenge within the fewest clicks. Our implementation uses a min-Heap priority queue and thus has a time complexity of O(V + Elog(V)), where V represents the number of vertices and E represents the number of edges.

We tested this algorithm by writing comprehensive test cases on the “small” and “medium” datasets that our group created by ourselves. We included the simple, easy test cases, along with the test cases that cover the cases when there are cycles and disconnected components as well, to prove that our algorithm works as expected under any circumstances. For the “whole” dataset, the dataset is very big, so we decided to pick pairs of nodes that we know the “expected” output (i.e. the “correct” output calculated by hand) to test our algorithms. We managed to get a few very “random” pairs, and yet, our function is still working as expected. Apart from the algorithm itself, we also tested that inputting as strings (page names) and inputting as integers (page IDs) both work as expected, to accommodate the user’s needs.

## Kosaraju’s
In the proposal, we ask a question about expanding the competition by defining sets of links that can be used in the competition that prevent players from “unfixable mistakes”. In each set, we guarantee that players won’t get stuck if they made a “bad move” since all of the nodes are strongly connected. Our Kosaraju’s algorithm takes in only the path to output file as its input, then output all of the possible sets of links described above, and also write the result to the specified output file. Our implementation uses DFS implementation and thus has a time complexity of O(V + E), where V represents the number of vertices and E represents the number of edges. 

The testcase for small and medium graphs, that includes cycles and disconnected components edge cases, are straightly worked out. However, the testcases that we performed on our entire dataset would only include only the first 800000 edges (~65000 nodes) of the dataset. This is because our Wikipedia dataset exceeds the limit of this algorithm, and running the algorithm on the entire dataset would have a segmentation fault because of stack overflowing from recursive calls. Yet, our test cases still show that the algorithm works correctly on general datasets (with smaller sizes). Additionally, since it is impossible to manually list out all the strongly connected components in such large graph, the test cases check whether some selected pairs of nodes that can surely be accessible from one another existed in the same strongly connected component.

## Constructor
Although the constructor is not an “algorithm” it played a big role in making our other algorithm functions (BFS, Djikstra’s, and Kosaraju’s) work.

The main idea of the constructor was to take an entire data set with nearly 2 million nodes from multiple .txt files and create the adjacency list with corresponding outgoing edge weights for each node. We also created two other maps that contained information from the .txt data files that we imported that allowed our algorithm functions to look up the different hyperlinks in constant time. 

We confirmed that the constructor is working as intended by adding test cases using a “small” and “medium” dataset that our team created, whose results we can manually confirm. We also wrote test cases for the full-scale dataset to check that our adjacency list output is as expected and that our helper maps that the algorithms use are mapping the right nodes to the right hyperlinks and vice versa. We also utilize programming techniques that check for unexpected inputs.

## Leading Question
By implementing the algorithms described above, we were able to accomplish our leading question/goal from our project proposal:

“From the dataset given, we select a starting Wikipedia page and a destination Wikipedia page. The goal is to navigate from the starting page to the destination page solely by clicking the links embedded within each page using the fewest clicks.”

### How did we answer this question? 
Dijkstra’s algorithm gives the user the shortest number of clicks to go from a starting Wikipedia link to a destination Wikipedia link that they can choose in the command line prompt. If the user out of curiosity wanted to find if it was possible to go from one Wikipedia link to another they could also do that through our Breadth First Search (BFS) algorithm. But that's not all! Suppose a user wanted to host a WikiRace competition, they can do so by using Kosaraju’s algorithm. The algorithm outputs a list of Wikipedia pages that the host can use in the competition that will guarantee that participants don’t stuck on a page that cannot reach their destination if they made bad moves. The combination of these features allows the user to have a robust guide through the WikiRace game and through the specific links of the dataset that we used both as a competitor and a host.
 
### What we discovered? 
One of the discoveries that our group made in this project was the amount of overlap between Wikipedia links amongst topics that are deemed to be unconnected to each other. Our Kosaraju’s algorithm indicates that there are many connections from one link to others that at first glance would never seem possible, and our Breadth-First Search traversal algorithm support this discovery because it only goes through an input link to a singular destination link or any destination link for that matter outputed from Kosaraju’s. Through this, we also discovered how easy and quick it was to go from one link to another completely unrelated link through Dijkstra’s. In the actual WikiRace game, suppose you were asked to go from a sperm capsule’s Wikipedia link to a cinema’s Wikipedia link as below.

[en.wikipedia.org/wiki/Spermatophore](en.wikipedia.org/wiki/Spermatophore) to [en.wikipedia.org/wiki/Cinema_4D](en.wikipedia.org/wiki/Cinema_4D)  

I bet you would be surprised to hear that you could go to the destination link with 4 clicks by going from Spermatophore’s page through Portable Document Format’s, Universal 3D’s, ArchiCAD’s, and finally Cinema 4D’s pages.

### What was the success of our project? 
Our group completed everything we had originally planned to complete from the project proposal. However, for Kosaraju’s algorithm, there’s a limit to how big the dataset can be (or else it would cause a SEGFAULT stack overflow error as explained above). Yet, the limit is still pretty large and should work for many general datasets. We all worked together correctly, utilizing all the different algorithms to create a final product that we are all very content with. 
